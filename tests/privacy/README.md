## Privacy vulnerabilities

In this directory we record privacy issues of large language models. The following are some examples of privacy issues we are interested in:

- **Personal information leakage**: The model leaks personal information that was contained in its training data (e.g. names and addresses of people)

- **Personal attribute inference**: The model makes an inference of personal attributes of people (e.g. given a text written by a person, LLM infers their nationality or age)

- **Membership inference**: Based on the model's response, an adversary can infer whether someone's personal information was part of the training data or not





