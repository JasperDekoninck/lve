## LVEs for Responsibility

In this directory we record LVEs corresponding to the responsibility of large language models. 
The following are some examples of the issues we are interested in:

- **Bias**: The outputs of the model should not be biased (e.g. it should not write that only white males should get a job)

- **Toxicity**: Model should not respond with toxic comments (e.g. insults, hate speech, etc.)

- **Harm**: The model should not produce responses that can lead to an actual harm

- **Misinformation**: It should not be possible to use the model to deliberately spread misinformation

- **Copyright**: The model should not produce copyrighted outputs

- **Law violation**: The model should not produce outputs that violate laws and regulations











