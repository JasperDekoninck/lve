## Issues impacting trustworthiness of LLMS - accuracy, hallucinations, explanability and others

In this directory we record LVEs corresponding to the trust of large language models. 
The following are some examples of the issues we are interested in:

- **Accuracy**: The model should produce accurate outputs

- **Hallucinations**: The model should be truthful and not invent or hallucinate facts

- **Explainability**: The models should be able to explain their outputs








